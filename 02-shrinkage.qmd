---
title: "Shrinkage and the correlation parameter"
jupyter: julia-1.8
---

```{julia}
#| include: false
using Pkg
Pkg.activate(".");
```

## What does omiting the correlation parameter do?

Because the number of correlations grows quadratically with the number of random slopes, adding just one additional slope (whether main effect or interaction) can greatly increase the number of free parameters in the model.
We can omit them from the model, using the `||` in lme4, splitting elements into separate `(x + ... | grp)` terms in lme4 or MixedModels.jl or using `zerocorr` in MixedModels.jl.

In terms of philosophy, this is a bit like omitting higher order interactions from the fixed effects: there is change in the *bias-variance tradeoff*.
However, practice suggests that the tradeoff is often worthwhile, although it makes shrinkage less efficient.
John Kruschke has [a nice worked example on his blog.](https://doingbayesiandataanalysis.blogspot.com/2019/07/shrinkage-in-hierarchical-models-random.html)

For our sleep study example, we we can see that there is very little impact because there is almost no correlation between the random intercept and random slope.

```{julia}
using CairoMakie
using MixedModels
using MixedModelsMakie

sleepstudy = MixedModels.dataset(:sleepstudy)
# REML=false by default in Julia
m2 = fit(MixedModel, @formula(reaction ~ 1 + days + (1 + days | subj)), sleepstudy)
```

We can see this with a shrinkage plot, which show the by-group (here: by-subject) offsets from the grand mean for each random effect. The red dots correspond to the esimtates you would get from classical linear regression within subjects, while the blue dots correspond to the shrunken "estimates" (technically predictions) you get for each subject from the mixed model.

```{julia}
shrinkageplot(m2)
```

```{julia}
MixedModels.PCA(m2)[:subj]
```

```{julia}
m2_zerocorr = fit(MixedModel, @formula(reaction ~ 1 + days + zerocorr(1 + days | subj)), sleepstudy)
```

```{julia}
shrinkageplot(m2_zerocorr)
```

```{julia}
MixedModels.PCA(m2)[:subj]
```

If we consider a more complex model, then the change can be much more dramatic:

```{julia}
kb07 = MixedModels.dataset(:kb07)
contrasts = Dict(:subj => Grouping(),
                 :item => Grouping(),
                 :spkr => EffectsCoding(),
                 :prec => EffectsCoding(),
                 :load => EffectsCoding() )
m_kb07 = fit(MixedModel, @formula(rt_raw ~ 1 + spkr * prec * load + (1 + spkr + prec + load|subj) + (1 + spkr + prec + load|item)), kb07; contrasts)
```

```{julia}
MixedModels.PCA(m_kb07)[:subj]
```

The effective dimensionality can be seen in the way that the random effects collapse into lines (i.e.. a 1-D object) within the majority of the panels (each representing a 2-D plane).
```{julia}
shrinkageplot(m_kb07, :subj)
```

```{julia}
m_kb07zc = fit(MixedModel, @formula(rt_raw ~ 1 + spkr * prec * load + zerocorr(1 + spkr + prec + load | subj) + zerocorr(1 + spkr + prec + load | item)), kb07; contrasts)
```

```{julia}
MixedModels.PCA(m_kb07zc)[:subj]
```

When we force the correlations to be zero, we can no longer get diagonal lines -- we we can only get horizontal or vertical lines within each panel.
Diagonal lines correspond to non zeor correlations between two variance components.
```{julia}
shrinkageplot(m_kb07zc, :subj)
```
