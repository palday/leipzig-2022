[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mixed-effects models: All the questions you were too afraid to ask",
    "section": "",
    "text": "Mixed-effects models have largely superseded classical repeated-measures ANOVA and paired t-tests in psychology and cognitive science and are quickly gaining ground in (cognitive) neuroscience. The underlying paradigm shift however has left many researchers with a number of questions, both general and domain specific. In this talk, I will cover a few core ideas underlying the application of mixed models and point toward other resources for more detailed follow-ups."
  },
  {
    "objectID": "index.html#general",
    "href": "index.html#general",
    "title": "Mixed-effects models: All the questions you were too afraid to ask",
    "section": "General",
    "text": "General\n\nWhat do all these convergence warnings mean? Should I worry about them?\nSingularities: Criteria when it is safe to ignore?\nWhy should I even care about mixed models? Isn’t ANOVA good enough?\nCan you please contrast the outcome of a simple ANOVA with the outcome of a linear mixed model for one and the same data set?\n\ncheck out https://vasishth-statistics.blogspot.com/2018/04/a-little-known-fact-paired-t-test-is.html\nANOVA is shortcut to model comparison and the choice of Type I vs. II vs. III sums of squares is a choice of which models you’re comparing\nhttps://lindeloev.github.io/tests-as-linear/\n\nWhen do we use the forward selection (i.e., drop-one-term) and when the backward selection (i.e., add-one-term strategy) during model fitting?\nBest strategy for model selection? This seems to be almost a question of ideology, top-down, bottom-up, theory derived only…\nBest way to estimate power?\n\nsimulation!\nsimr in R\nMixedModelsSim in Julia\nsee also https://github.com/palday/freiburg2022/\nsee also https://gitlab.com/palday/precision-is-the-goal/-/blob/master/presentation.md\n\nHow to compute Bayes Factors with lmer models (so far we use https://rpubs.com/lindeloev/bayes_factors; is this approach correct/optimal?)\n\nthis is really tough!"
  },
  {
    "objectID": "index.html#assumptions-and-violations-thereof",
    "href": "index.html#assumptions-and-violations-thereof",
    "title": "Mixed-effects models: All the questions you were too afraid to ask",
    "section": "Assumptions and violations thereof",
    "text": "Assumptions and violations thereof\n\nPartly from reviewer perspective: Violations of distribution assumptions, how vulnerable are LMMs in practice?\n\nIt depends…\nSee here for some slides\nBottom line: standard errors are the first thing to go when the residual error isn’t anywhere near normal\nNB: the majority of assumptions are on the conditional distribution, i.e. the distribution of the residuals, not the marginal distribution (the “raw” distribution of the data)\n\nMulticollinearity: How bad can it be?\n\ngenerally speaking, multicollinearity inflates your standard errors and so consumes statistical power\nThere are even arguments against using tricks like residualization to compensate for multicollinearity and instead for collecting more data to compensate\nthe variance inflation factor attempts to quantify the amount that the standard errors are inflated\npredictions based on your model aren’t really impacted by multicollinearity because any perturbation of one coefficient pulls its interwined coefficient along\nnear perfect multicollinearity can nonetheless cause numerical problems\n\nHow to analyze RTs with (G)LMMs (skewed distributions)?\n\nLo S and Andrews S (2015) To transform or not to transform: using generalized linear mixed models to analyse reaction time data. Front. Psychol. 6:1171. doi: 10.3389/fpsyg.2015.01171\nlook at speed instead of RT – theories are often equally easy to formulate as speed (“participants are faster in condition A”)\nAlso checkout the general category of Box-Cox transformations\n\nHow to model heteroskedasticity in (G)LMM?\n\nin lme4/MixedModels.jl – with some difficulty\nnlme, glmmTMB and brms offer better support for this\nbut make sure that you really need it!\n\nIs there a suitable link function?\n\ndo you need a link function or a transformation of the response?"
  },
  {
    "objectID": "index.html#contrast-coding-and-standardizing",
    "href": "index.html#contrast-coding-and-standardizing",
    "title": "Mixed-effects models: All the questions you were too afraid to ask",
    "section": "Contrast coding and standardizing",
    "text": "Contrast coding and standardizing\n\nTo standardize or not to standardize?\n\nwhatever gives a natural interpretation!\ncentering is generally a good idea unless the original scale has a meaningful “natural” zero (see the documentaiton of StandardizedPredictors.jl for a nice example)\n\nDifferent codings (dummy vs. effects vs. …): What to use when and what can go wrong?\n\nthis is part of why visualization with the effects package in R or Effects.jl in Julia can be quite helpful\nBrehm, L., Alday, P. M., (2022). “Contrast coding choices in a decade of mixed models.” Journal of Memory and Language 125, p. 104334. DOI: 10.1016/j.jml.2022.104334 URL: https://osf.io/jkpxt/\nSchad, D. J., Vasishth, S., Hohenstein, S., & Kliegl, R. (2020). How to capitalize on a priori contrasts in linear (mixed) models: A tutorial. Journal of Memory and Language, 110, 104038. https://doi.org/10.1016/j.jml.2019.104038\n\nWhat are the benefits and costs of ortshogonality of contrasts (and their implications for the random-effects structure)?\nHow do we determine the correct order of polynomial trends (and why do we need to find out to being with)?"
  },
  {
    "objectID": "index.html#random-effects",
    "href": "index.html#random-effects",
    "title": "Mixed-effects models: All the questions you were too afraid to ask",
    "section": "Random effects",
    "text": "Random effects\n\nWhat are random effects actually?\nHow do I choose the correct random effects structure for my model + data?\nWhat are the the consequences of misspecifying the random effects structure?\nHow to properly use RE PCA (for example how to identify the effects)?\nWhat does zerocorr in MixedModels.jl / || in lme4 do? What does it mean for interpreting my data?\nShould we first remove variance components for interaction terms or correlation parameters when selecting a model?"
  },
  {
    "objectID": "index.html#eeg-erp",
    "href": "index.html#eeg-erp",
    "title": "Mixed-effects models: All the questions you were too afraid to ask",
    "section": "EEG / ERP",
    "text": "EEG / ERP\n\nHow do I handle EEG electrodes in mixed models? Are they fixed or random effects?\nCan we model single-trial ERP data? Is there anything special to consider here?\n\nYes, we can!\nThe biggest challenge is appropriate selection of temporal / spatial ROIs and how to model timecourses/topography\nKretzschmar, F., Alday, P. M., (submitted). “Principles of statistical analysis: old and new tools.” In: Language Electrified. Techniques, Methods, Applications, and Future Perspectives in the Neurophysiological Investigation of Language. Ed. by Grimaldi, Mirko, Shtyrov, Yury, and Brattico, Elvira. DOI: 10.31234/osf.io/nyj3k\n\nWe would like to model single-trial PCA sores projected from group PCA loadings for ERP data. Would you consider this a valid approach?\n\nYes, I think this could be a quite interesting approach, though I might consider ICA instead of PCA."
  },
  {
    "objectID": "01-convergence.html",
    "href": "01-convergence.html",
    "title": "Fitting a mixed model: convergence and singularity",
    "section": "",
    "text": "Likelihood: the probability of the data given the model\nGiven a model specification, find the values of the model parameters that maximizes the likelihood\nFind the version of specified model that best matches observed data\n\n\nYou need to remember that “likelihood” is a technical term. The likelihood of \\(H\\), \\(Pr(O\\|H)\\), and the posterior probability of \\(H\\), \\(Pr(H\\|O)\\), are different quantities and they can have different values. The likelihood of \\(H\\) is the probability that \\(H\\) confers on \\(O\\), not the probability that \\(O\\) confers on \\(H\\). Suppose you hear a noise coming from the attic of your house. You consider the hypothesis that there are gremlins up there bowling. The likelihood of this hypothesis is very high, since if there are gremlins bowling in the attic, there probably will be noise. But surely you don’t think that the noise makes it very probable that there are gremlins up there bowling. In this example, \\(Pr(O\\|H)\\) is high and \\(Pr(H\\|O)\\) is low. The gremlin hypothesis has a high likelihood (in the technical sense) but a low probability.\n\nSober, E. (2008). Evidence and Evolution: the Logic Behind the Science. Cambridge University Press.\n\n\n\n\nREML: Residualized Maximum Likelihood\n\nNot actually the likelihood\nRelated to the \\(n\\) vs. \\(n-1\\) in e.g. calculating SDs\n\nIf your sample sizes are small enough that this matters, then your sample sizes are too small.\n\n\nDeviance:\n\n“\\(-2 \\log \\text{Likelihood}\\)”\n(up to an additive constant, due to the difficulty in defining the fully saturated model required for the deviance)\nso maximizing the likelihood is the same as minimizing the deviance\nthe difference in the deviance is the \\(\\chi^2\\) value in the likelihood ratio test computed by anova() in R\n\ndifferences on the log scale are the same as ratios on the original scale\nthe additive constant cancels out\n\n\n\n\n\n\n\nOnce the REML/ML/deviance criterion is defined, we just need to find the value maximizes (REML,ML) / minimizes it (deviance).\nIt turns out that this problem only depends on the random effects and not on the fixed effects! (More precisely, for a given value of the random effects, we can directly compute the value of the fixed effects that maximizes the likelihood.)\nThis is called function optimization.\nThere are many techniques for this and many different implementations of each technique.\nThere are no free lunches and no optimizer which will always work the best.\nThis is why lme4 allows you to pick your optimizer and change its settings.\nChanges in the default optimization settings are often the cause of new warnings when updating your lme4 version.\nFor more information, see:\n\n?convergence\n?lmerControl\n\nOne bit of fine print: for numerical reasons, everything is scaled relative to the residual variance during the fitting process (but this is hidden from the user).\n\n\n\n\n\nlibrary(\"lme4\")\n\nLoading required package: Matrix\n\nlibrary(\"lattice\")\nxyplot(Reaction ~ Days | Subject, sleepstudy, type = c(\"g\",\"p\",\"r\"),\n       index = function(x,y) coef(lm(y ~ x))[1],\n       xlab = \"Days of sleep deprivation\",\n       ylab = \"Average reaction time (ms)\", aspect = \"xy\")\n\n\n\n\n\n\n\nLet’s consider the intercept-only model, i.e. a model with only one variance component.\n\nm <- lmer(Reaction ~ 1 + Days + (1|Subject), data=sleepstudy, REML=FALSE)\nsummary(m)\n\nLinear mixed model fit by maximum likelihood  ['lmerMod']\nFormula: Reaction ~ 1 + Days + (1 | Subject)\n   Data: sleepstudy\n\n     AIC      BIC   logLik deviance df.resid \n  1802.1   1814.9   -897.0   1794.1      176 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.2347 -0.5544  0.0155  0.5257  4.2648 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n Subject  (Intercept) 1296.9   36.01   \n Residual              954.5   30.90   \nNumber of obs: 180, groups:  Subject, 18\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) 251.4051     9.5062   26.45\nDays         10.4673     0.8017   13.06\n\nCorrelation of Fixed Effects:\n     (Intr)\nDays -0.380\n\n\n\n#| echo: false\ntheta <- getME(m,\"theta\")\nprint(theta)\n\nSubject.(Intercept) \n           1.165612 \n\nprint(VarCorr(m))\n\n Groups   Name        Std.Dev.\n Subject  (Intercept) 36.012  \n Residual             30.895  \n\nff <- as.function(m)\ntvec <- seq(0,2,length=101)\nLvec <- sapply(tvec,ff)\npar(bty=\"l\",las=1)\npar(lwd=2, cex=2)\npar(mar=c(4,4,1,2))\nplot(tvec,Lvec,type=\"l\",\n     ylab=\"Deviance\",\n     xlab=\"scaled random effects standard deviation\",\n     ylim=c(1750,1901))\npoints(theta,ff(theta),pch=16,col=1)\n\n\n\n\n\n\n\n\nm2 <- lmer(Reaction ~ 1 + Days + (1 + Days | Subject), data=sleepstudy, REML=FALSE)\n\nanova(m, m2)\n\nData: sleepstudy\nModels:\nm: Reaction ~ 1 + Days + (1 | Subject)\nm2: Reaction ~ 1 + Days + (1 + Days | Subject)\n   npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)    \nm     4 1802.1 1814.8 -897.04   1794.1                         \nm2    6 1763.9 1783.1 -875.97   1751.9 42.139  2  7.072e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nprint(VarCorr(m2))\n\n Groups   Name        Std.Dev. Corr \n Subject  (Intercept) 23.7798       \n          Days         5.7168  0.081\n Residual             25.5919       \n\n\nPartial image for intercept with constant (optimal) slope and correlation.\n\n#| echo: false\ntheta2 <- getME(m2,\"theta\")\nff2 <- function(x) as.function(m2)(c(x,theta2[2],theta2[3]))\ntvec2 <- seq(0,2,length=101)\nLvec2 <- sapply(tvec2,ff2)\npar(bty=\"l\",las=1)\npar(lwd=2, cex=2)\npar(mar=c(4,4,1,2))\nplot(tvec,Lvec,type=\"l\",\n     ylab=\"Deviance\",\n     xlab=\"scaled random effects standard deviation\",\n     ylim=c(1750,1901))\npoints(theta,ff(theta),pch=16,col=1)\nlines(tvec2,Lvec2,col=4,lty=2,pch=6)\npoints(theta2[1],as.function(m2)(theta2),pch=16,col=4)\ntext(1.25, 1820, \"Intercept only model\")\ntext(1.0, 1780, \"Intercept and slope model\", col=4)\n\n\n\n\n\n\n\n\nm3 <- lmer(Reaction ~ 1 + Days + (1 + Days || Subject), data=sleepstudy, REML=FALSE)\nsummary(m3)\n\nLinear mixed model fit by maximum likelihood  ['lmerMod']\nFormula: Reaction ~ 1 + Days + ((1 | Subject) + (0 + Days | Subject))\n   Data: sleepstudy\n\n     AIC      BIC   logLik deviance df.resid \n    1762     1778     -876     1752      175 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.9535 -0.4673  0.0239  0.4625  5.1883 \n\nRandom effects:\n Groups    Name        Variance Std.Dev.\n Subject   (Intercept) 584.27   24.172  \n Subject.1 Days         33.63    5.799  \n Residual              653.12   25.556  \nNumber of obs: 180, groups:  Subject, 18\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  251.405      6.708   37.48\nDays          10.467      1.519    6.89\n\nCorrelation of Fixed Effects:\n     (Intr)\nDays -0.194\n\n\n\n#| echo: false\ntheta3 <- getME(m3,\"theta\")\npar(lwd=2, cex=2)\npar(mar=c(4,4,1,2))\nff3 <- function(x,y) as.function(m3)(c(x,y))\nff3v <- Vectorize(ff3)\nxx <- seq(0,2,length=51)\nLvec3 <- outer(xx, xx, ff3v)\ncontour(xx,xx,Lvec3, nlevels=20,xlab=\"Intercept\",ylab=\"Days\",main=\"Deviance by Scaled RE SD\",labcex=1.5)\npoints(theta3[1],theta3[2],pch=16,col=6)"
  },
  {
    "objectID": "01-convergence.html#maximum-likelihood-estimation-mle",
    "href": "01-convergence.html#maximum-likelihood-estimation-mle",
    "title": "Fitting a mixed model: convergence and singularity",
    "section": "Maximum Likelihood Estimation (MLE)",
    "text": "Maximum Likelihood Estimation (MLE)\n\nLikelihood: the probability of the data given the model\nGiven a model specification, find the values of the model parameters that maximizes the likelihood\nFind the version of specified model that best matches observed data\n\n\nYou need to remember that “likelihood” is a technical term. The likelihood of \\(H\\), \\(Pr(O\\|H)\\), and the posterior probability of \\(H\\), \\(Pr(H\\|O)\\), are different quantities and they can have different values. The likelihood of \\(H\\) is the probability that \\(H\\) confers on \\(O\\), not the probability that \\(O\\) confers on \\(H\\). Suppose you hear a noise coming from the attic of your house. You consider the hypothesis that there are gremlins up there bowling. The likelihood of this hypothesis is very high, since if there are gremlins bowling in the attic, there probably will be noise. But surely you don’t think that the noise makes it very probable that there are gremlins up there bowling. In this example, \\(Pr(O\\|H)\\) is high and \\(Pr(H\\|O)\\) is low. The gremlin hypothesis has a high likelihood (in the technical sense) but a low probability.\n\nSober, E. (2008). Evidence and Evolution: the Logic Behind the Science. Cambridge University Press."
  },
  {
    "objectID": "01-convergence.html#some-additional-terminology",
    "href": "01-convergence.html#some-additional-terminology",
    "title": "Fitting a mixed model: convergence and singularity",
    "section": "Some additional terminology",
    "text": "Some additional terminology\n\nREML: Residualized Maximum Likelihood\n\nNot actually the likelihood\nRelated to the \\(n\\) vs. \\(n-1\\) in e.g. calculating SDs\n\nIf your sample sizes are small enough that this matters, then your sample sizes are too small.\n\n\nDeviance:\n\n“\\(-2 \\log \\text{Likelihood}\\)”\n(up to an additive constant, due to the difficulty in defining the fully saturated model required for the deviance)\nso maximizing the likelihood is the same as minimizing the deviance\nthe difference in the deviance is the \\(\\chi^2\\) value in the likelihood ratio test computed by anova() in R\n\ndifferences on the log scale are the same as ratios on the original scale\nthe additive constant cancels out"
  },
  {
    "objectID": "01-convergence.html#optimization",
    "href": "01-convergence.html#optimization",
    "title": "Fitting a mixed model: convergence and singularity",
    "section": "Optimization",
    "text": "Optimization\n\nOnce the REML/ML/deviance criterion is defined, we just need to find the value maximizes (REML,ML) / minimizes it (deviance).\nIt turns out that this problem only depends on the random effects and not on the fixed effects! (More precisely, for a given value of the random effects, we can directly compute the value of the fixed effects that maximizes the likelihood.)\nThis is called function optimization.\nThere are many techniques for this and many different implementations of each technique.\nThere are no free lunches and no optimizer which will always work the best.\nThis is why lme4 allows you to pick your optimizer and change its settings.\nChanges in the default optimization settings are often the cause of new warnings when updating your lme4 version.\nFor more information, see:\n\n?convergence\n?lmerControl\n\nOne bit of fine print: for numerical reasons, everything is scaled relative to the residual variance during the fitting process (but this is hidden from the user)."
  },
  {
    "objectID": "01-convergence.html#an-example",
    "href": "01-convergence.html#an-example",
    "title": "Fitting a mixed model: convergence and singularity",
    "section": "An example",
    "text": "An example\nlibrary(\"lme4\")\nlibrary(\"lattice\")\nxyplot(Reaction ~ Days | Subject, sleepstudy, type = c(\"g\",\"p\",\"r\"),\n       index = function(x,y) coef(lm(y ~ x))[1],\n       xlab = \"Days of sleep deprivation\",\n       ylab = \"Average reaction time (ms)\", aspect = \"xy\")"
  },
  {
    "objectID": "01-convergence.html#optimizing-an-intercept-only-model",
    "href": "01-convergence.html#optimizing-an-intercept-only-model",
    "title": "Fitting a mixed model: convergence and singularity",
    "section": "Optimizing an intercept-only model",
    "text": "Optimizing an intercept-only model\nLet’s consider the intercept-only model, i.e. a model with only one variance component.\n\nm <- lmer(Reaction ~ 1 + Days + (1|Subject), data=sleepstudy, REML=FALSE)\nsummary(m)\n\n#| echo: false\ntheta <- getME(m,\"theta\")\nprint(theta)\nprint(VarCorr(m))\n\nff <- as.function(m)\ntvec <- seq(0,2,length=101)\nLvec <- sapply(tvec,ff)\npar(bty=\"l\",las=1)\npar(lwd=2, cex=2)\npar(mar=c(4,4,1,2))\nplot(tvec,Lvec,type=\"l\",\n     ylab=\"Deviance\",\n     xlab=\"scaled random effects standard deviation\",\n     ylim=c(1750,1901))\npoints(theta,ff(theta),pch=16,col=1)"
  },
  {
    "objectID": "01-convergence.html#optimizing-a-model-with-intercept-slope-and-correlation",
    "href": "01-convergence.html#optimizing-a-model-with-intercept-slope-and-correlation",
    "title": "Fitting a mixed model: convergence and singularity",
    "section": "Optimizing a model with intercept, slope and correlation",
    "text": "Optimizing a model with intercept, slope and correlation\n\nm2 <- lmer(Reaction ~ 1 + Days + (1 + Days | Subject), data=sleepstudy, REML=FALSE)\n\nanova(m, m2)\nprint(VarCorr(m2))\nPartial image for intercept with constant (optimal) slope and correlation.\n\n#| echo: false\ntheta2 <- getME(m2,\"theta\")\nff2 <- function(x) as.function(m2)(c(x,theta2[2],theta2[3]))\ntvec2 <- seq(0,2,length=101)\nLvec2 <- sapply(tvec2,ff2)\npar(bty=\"l\",las=1)\npar(lwd=2, cex=2)\npar(mar=c(4,4,1,2))\nplot(tvec,Lvec,type=\"l\",\n     ylab=\"Deviance\",\n     xlab=\"scaled random effects standard deviation\",\n     ylim=c(1750,1901))\npoints(theta,ff(theta),pch=16,col=1)\nlines(tvec2,Lvec2,col=4,lty=2,pch=6)\npoints(theta2[1],as.function(m2)(theta2),pch=16,col=4)\ntext(1.25, 1820, \"Intercept only model\")\ntext(1.0, 1780, \"Intercept and slope model\", col=4)"
  },
  {
    "objectID": "01-convergence.html#optimizing-a-model-with-intercept-slope-and-no-correlation",
    "href": "01-convergence.html#optimizing-a-model-with-intercept-slope-and-no-correlation",
    "title": "Fitting a mixed model: convergence and singularity",
    "section": "Optimizing a model with intercept, slope and no correlation",
    "text": "Optimizing a model with intercept, slope and no correlation\n\nm3 <- lmer(Reaction ~ 1 + Days + (1 + Days || Subject), data=sleepstudy, REML=FALSE)\nsummary(m3)\n\n#| echo: false\ntheta3 <- getME(m3,\"theta\")\npar(lwd=2, cex=2)\npar(mar=c(4,4,1,2))\nff3 <- function(x,y) as.function(m3)(c(x,y))\nff3v <- Vectorize(ff3)\nxx <- seq(0,2,length=51)\nLvec3 <- outer(xx, xx, ff3v)\ncontour(xx,xx,Lvec3, nlevels=20,xlab=\"Intercept\",ylab=\"Days\",main=\"Deviance by Scaled RE SD\",labcex=1.5)\npoints(theta3[1],theta3[2],pch=16,col=6)"
  },
  {
    "objectID": "01-convergence.html#optimization-failures-i-failing-post-optimization-tests",
    "href": "01-convergence.html#optimization-failures-i-failing-post-optimization-tests",
    "title": "Fitting a mixed model: convergence and singularity",
    "section": "Optimization failures I: Failing post-optimization tests",
    "text": "Optimization failures I: Failing post-optimization tests\n\nOptimizers have ways to detect when they have reached an optimum.\nIf they didn’t, they wouldn’t know how to stop!\nHowever, optimization may be stopped prematurely for various reasons :\n\nlimit on number of function evaluations (maxeval)\nnot enough improvement in deviance with additional steps (ftol_abs, ftol_rel)\nnot enough change in theta / RE estimates with additional steps (xtol_abs, xtol_rel)\n\nTrying to different options or even a different optimizer are good strategies.\nIf an optimizer thinks that it has converged, then that’s generally the best convergence diagnostic.\nThere are a few post-hoc tests we can run, but they are not guaranteed to be accurate.\n\nOption names are all for nloptwrap, the current default lmer() optimizer. glmer still uses bobyqa, which has different names …."
  },
  {
    "objectID": "01-convergence.html#checking-derivatives",
    "href": "01-convergence.html#checking-derivatives",
    "title": "Fitting a mixed model: convergence and singularity",
    "section": "Checking derivatives",
    "text": "Checking derivatives\n\nThe gradient is a higher dimensional generalization of the derivative or instaneous rate of change.\nIn lower dimensions, we can visualize this as the slope of a tangent line at a given point.\nWhen we’ve arrived at an optimum not at the boundary, then this slope is zero.\nBUT this check is slow and inaccurate\n\nthe gradient is approximated by finite differences\nwhich is not particularly accurate in its own right\nand involves lots of slow function evaluations\nand is scale dependent\n\n\n\n#| echo: false\npar(lwd=2, cex=2)\npar(mar=c(4,4,1,2))\nplot(tvec2,Lvec2,type=\"l\",\n     ylab=\"Deviance\",\n     xlab=\"scaled random effects standard deviation\",\n     ylim=c(1748,1775))\npoints(theta2[1],as.function(m2)(theta2),pch=16,col=4)\narg_min <- which.min(Lvec2)\nb <- ((Lvec2[arg_min+1]-Lvec2[arg_min]) + (Lvec2[arg_min]-Lvec2[arg_min-1])) / 2\n\na <- Lvec2[arg_min] - b * tvec2[arg_min]\nabline(a=a,b=b,lty=2)\ntext(tvec2[arg_min],1750,label=sprintf(\"Estimated slope: %f\", b))"
  },
  {
    "objectID": "01-convergence.html#equality-is-not-what-it-seems",
    "href": "01-convergence.html#equality-is-not-what-it-seems",
    "title": "Fitting a mixed model: convergence and singularity",
    "section": "Equality is not what it seems",
    "text": "Equality is not what it seems\n\n0.1 + 0.1 + 0.1 - 0.3\n\n[1] 5.551115e-17\n\n\n\nComputers don’t store infinite digits and internally use a form of scientific notation (floating point, 1e-16 => \\(1 \\times 10^{-16}\\)).\nExact equality doesn’t hold in the presence of rounding.\nFlat deviance curves may not be numerically flat with coarse approximations.\nSimilarly, Numerically flat curves may not be actually flat ."
  },
  {
    "objectID": "01-convergence.html#recommendations-for-warnings-about-the-gradient-and-hessian",
    "href": "01-convergence.html#recommendations-for-warnings-about-the-gradient-and-hessian",
    "title": "Fitting a mixed model: convergence and singularity",
    "section": "Recommendations for warnings about the gradient and Hessian",
    "text": "Recommendations for warnings about the gradient and Hessian\n\nTry setting stricter convergence criteria and allowing more iterations (ftol_rel, xtol_rel, maxeval).\nCheck your overall model fit by plotting fitted vs. observed, etc. to make sure your model isn’t misspecified.\nConsider setting control=lmerControl(calc.derivs=FALSE).\n\nThe derivative check takes a loooooong time\nAnd tends to deliver false positives with maximal models.\n\nBUT PAY ATTENTION TO YOUR MODEL FIT AND YOUR OPTIMIZER’S OWN WARNINGS!!!\nNote that warnings about convergence failure, e.g. a Hessian that is not positive definite, may still indicate genuine problems – or that you should at least consider rescaling, as the warnings tell you to do.\nIn MixedModels.jl, we only use the optimizer’s own checks and do not perform any post-hoc checks."
  },
  {
    "objectID": "01-convergence.html#optimization-failures-ii",
    "href": "01-convergence.html#optimization-failures-ii",
    "title": "Fitting a mixed model: convergence and singularity",
    "section": "Optimization failures II",
    "text": "Optimization failures II\nSuccessfully completed optimization may not be the global optimum, but only a local one.\n\n#| echo: false\n\n# from https://stats.stackexchange.com/questions/384528/lme-and-lmer-giving-conflicting-results/384539#384539\n\nset.seed(21)\nHeight=1:10; Height=Height+runif(10,min=0,max=3) #First height measurement\nWeight=1:10; Weight=Weight+runif(10,min=0,max=3) #First weight measurement\n\nHeight2=Height+runif(10,min=0,max=1) #second height measurement\nWeight2=Weight-runif(10,min=0,max=1) #second weight measurement\n\nHeight=c(Height,Height2) #combine height and wight measurements\nWeight=c(Weight,Weight2)\n\nDF=data.frame(Height,Weight) #generate data frame\nDF$ID=as.factor(rep(1:10,2)) #add subject ID\nDF$Number=as.factor(c(rep(1,10),rep(2,10))) #difference\n\n\n#| echo: false\npar(lwd=2, cex=2)\npar(mar=c(4,4,1,2))\nplot(Weight,Height)\nsegments(Weight,Height,Weight2,Height2)\n\n\n\n\nadapted from https://stats.stackexchange.com/questions/384528/lme-and-lmer-giving-conflicting-results/\n\nminqa <- lmer(Height~Weight+(1|ID),\n              data=DF,\n              control=lmerControl(optimizer=\"bobyqa\"),\n              REML=FALSE)\nnloptwrap <- lmer(Height~Weight+(1|ID),\n              data=DF,\n              control=lmerControl(optimizer=\"nloptwrap\"),\n              REML=FALSE)\n\n\n#| echo: false\npar(lwd=2, cex=2)\npar(mar=c(4,4,1,2))\nplot(Weight,Height)\nsegments(Weight,Height,Weight2,Height2)\nabline(fixef(minqa)[1],fixef(minqa)[2],col=2)\nabline(fixef(nloptwrap)[1],fixef(nloptwrap)[2],col=1)\nlegend(2,13,c(\"bobyqa\",\"nloptwrap\"),col=c(2,1),lty=1)\n\n\n\n\n\n#| echo: false\npar(lwd=2, cex=2)\npar(mar=c(4,4,1,2))\nsd.minqa <- getME(minqa,\"theta\")\nsd.nloptwrap <- getME(nloptwrap,\"theta\")\n\nff <- as.function(minqa)\ntvec <- seq(0,20,length=101)\nLvec <- sapply(tvec,ff)\npar(bty=\"l\",las=1)\nplot(tvec,Lvec,type=\"l\",\n     ylab=\"Deviance\",\n     xlab=\"scaled random effects standard deviation\",\n     lwd = 2)\n#abline(v=1,lty=2)\npoints(sd.minqa,ff(sd.minqa),pch=16,col=2)\npoints(sd.nloptwrap,ff(sd.nloptwrap),pch=16,col=1)\nlegend(10,76,c(\"bobyqa\",\"nloptwrap\"),col=c(2,1),pch=16)"
  },
  {
    "objectID": "01-convergence.html#which-n-counts",
    "href": "01-convergence.html#which-n-counts",
    "title": "Fitting a mixed model: convergence and singularity",
    "section": "Which n counts?",
    "text": "Which n counts?\n\nFor a 2x2 within-subject design with “maximal” random slopes, we have 10 parameters!\n\n4 standard deviations\n\n1 intercept\n2 main effects\n1 interaction\n\n6 correlations\n\n(4 SDs * 3 other SDs) / 2 because order doesn’t matter\n\n\nWith typical 20-30 participant designs, we only have 20-30 levels of the grouping variable\nSo we only have 2-3 “samples” per parameter we’re trying to estimate – expect noisy estimates and overfitting!\nFor a 2x2x2 design, we have more parameters (34) than samples!\n(The same math holds for item RE.)\nIt is not the total number of observations that matters for RE but rather the number of levels of the grouping variable!"
  },
  {
    "objectID": "01-convergence.html#boundary-fits",
    "href": "01-convergence.html#boundary-fits",
    "title": "Fitting a mixed model: convergence and singularity",
    "section": "Boundary fits",
    "text": "Boundary fits\n\nOne of the innovations of lme4 compared to its predecessor nlme is its ability to fit models with optima at the boundaries, e.g. when a random effect goes to zero.\nNote that this does not mean that there is no between participant/group variation, but rather that there is no variation not captured by the residual variation.\nThis lack of variation beyond the residual variation may just be due to lack of power to detect it!"
  },
  {
    "objectID": "01-convergence.html#boundary-fits-are-singular-fits",
    "href": "01-convergence.html#boundary-fits-are-singular-fits",
    "title": "Fitting a mixed model: convergence and singularity",
    "section": "Boundary fits are singular fits",
    "text": "Boundary fits are singular fits\n\nNumerically, boundary fits have non-invertible, i.e. singular matrices.\nOne or more of the REs being numerically zero is one possibility; however, not the only one.\nIf any linear combination of REs is zero, then this also yields a singular / boundary fit. For a geometric interpretation, think of a sloping ceiling – you may not be able to get to the edge of the floor nor the complete top of the building, but the combination of constraints leading to you hitting an edge.\nSingular fits are not problematic per se – they are mathematically well-defined and it is possible that the singular fit is actually the best description of the data.\nHowever, they are often indicative of overfitting, e.g. because you don’t have enough data to distinguish group variation from residual variation.\nThis overfitting, or equivalently, lack of power is a problem.\nSingular fits are also slow to compute. So simplify your model or gather more data."
  },
  {
    "objectID": "01-convergence.html#maximal-models-boundaries-and-gradients-a-nasty-interaction",
    "href": "01-convergence.html#maximal-models-boundaries-and-gradients-a-nasty-interaction",
    "title": "Fitting a mixed model: convergence and singularity",
    "section": "Maximal models, boundaries and gradients: a nasty interaction",
    "text": "Maximal models, boundaries and gradients: a nasty interaction\n\n#| echo: false\n\nlibrary(\"nlme\")\n\n\nAttaching package: 'nlme'\n\n\nThe following object is masked from 'package:lme4':\n\n    lmList\n\nset.seed(21)\nWeight=1:10; Weight=Weight+runif(10,min=0,max=3) #First weight measurement\nWeight = c(Weight, Weight)# + runif(10,min=-1e-5,max=1e-5))\nHeight = 0.8017481 * Weight + 2.33605 + rnorm(20,mean=0,sd=1)\n\nDFsing=data.frame(Height,Weight) #generate data frame\nDFsing$ID=as.factor(rep(1:10,2)) #add subject ID\nDFsing$Number=as.factor(c(rep(1,10),rep(2,10))) #difference\n\nmsingular = lmer(Height~Weight+(1|ID),data=DFsing,\n                 REML=FALSE,\n                 control=lmerControl(optimizer=\"nloptwrap\",\n                                     calc.derivs = FALSE))\n\nboundary (singular) fit: see help('isSingular')\n\nsummary(msingular)\n\nLinear mixed model fit by maximum likelihood  ['lmerMod']\nFormula: Height ~ Weight + (1 | ID)\n   Data: DFsing\nControl: lmerControl(optimizer = \"nloptwrap\", calc.derivs = FALSE)\n\n     AIC      BIC   logLik deviance df.resid \n    60.9     64.9    -26.5     52.9       16 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.2393 -0.8435  0.2213  0.6692  1.6312 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n ID       (Intercept) 0.0000   0.0000  \n Residual             0.8251   0.9083  \nNumber of obs: 20, groups:  ID, 10\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  2.53589    0.50572   5.014\nWeight       0.75121    0.06368  11.797\n\nCorrelation of Fixed Effects:\n       (Intr)\nWeight -0.916\noptimizer (nloptwrap) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\nmsinglme=lme(Height~Weight,random=~1|ID,data=DFsing, method=\"ML\")\nff <- as.function(msingular)\n\ntvec <- seq(0,5,length=101)\nLvec <- sapply(tvec,ff)\npar(bty=\"l\",las=1)\nsdsing <- getME(msingular,\"theta\")\nsdsinglme <- sqrt(getVarCov(msinglme)[[1]])/sigma(msinglme)\n\n\n#| echo: false\npar(lwd=2, cex=2)\npar(mar=c(4,4,1,2))\nplot(tvec,Lvec,type=\"l\",\n     ylab=\"deviance\",\n     xlab=\"scaled random effects standard deviation\")\npoints(sdsinglme,ff(sdsing),pch=16,col=4)\npoints(sdsing,ff(sdsing),pch=16,col=2)\ntext(3,60,label=sprintf(\"nlme: %f\\nlme4: %f\", sdsinglme,sdsing))"
  },
  {
    "objectID": "01-convergence.html#repca",
    "href": "01-convergence.html#repca",
    "title": "Fitting a mixed model: convergence and singularity",
    "section": "rePCA",
    "text": "rePCA\n\nrePCA() performs PCA on the random-effects matrix.\nThis is useful for determining the effective dimensionality of our RE – in other words, how many RE terms are supported by your data.\nFor the sleepstudy, we could get 96% of the variance explained with one RE term!\n\n\nsummary(rePCA(m2))\n\n$Subject\nImportance of components:\n                         [,1]    [,2]\nStandard deviation     0.9294 0.22260\nProportion of Variance 0.9457 0.05425\nCumulative Proportion  0.9457 1.00000\n\n\n\nsee Bates et al. (2015), “Parsimonious Mixed Models”."
  },
  {
    "objectID": "01-convergence.html#what-does-omiting-the-correlation-parameter-do",
    "href": "01-convergence.html#what-does-omiting-the-correlation-parameter-do",
    "title": "Fitting a mixed model: convergence and singularity",
    "section": "What does omiting the correlation parameter do?",
    "text": "What does omiting the correlation parameter do?\nBecause the number of correlations grows quadratically with the number of random slopes, adding just one additional slope (whether main effect or interaction) can greatly increase the number of free parameters in the model. We can omit them from the model, using the || in lme4, splitting elements into separate (x + ... | grp) terms in lme4 or MixedModels.jl or using zerocorr in MixedModels.jl.\nIn terms of philosophy, this is a bit like omitting higher order interactions from the fixed effects: there is change in the bias-variance tradeoff. However, practice suggests that the tradeoff is often worthwhile, although it makes shrinkage less efficient. John Kruschke has a nice worked example on his blog.\nFor our sleep study example, we we can see that there is very little impact because there is almost no correlation between the random intercept and random slope.\n\nusing CairoMakie\nusing MixedModels\nusing MixedModelsMakie\n\nsleepstudy = MixedModels.dataset(:sleepstudy)\n# REML=false by default in Julia\nm2 = fit(MixedModel, @formula(reaction ~ 1 + days + (1 + days | subj)), sleepstudy)\n\n\n\n\n\nEst.\nSE\nz\np\nσ_subj\n\n\n\n\n(Intercept)\n251.4051\n6.6323\n37.91\n<1e-99\n23.7805\n\n\ndays\n10.4673\n1.5022\n6.97\n<1e-11\n5.7168\n\n\nResidual\n25.5918\n\n\n\n\n\n\n\n\n\nWe can see this with a shrinkage plot, which show the by-group (here: by-subject) offsets from the grand mean for each random effect. The red dots correspond to the esimtates you would get from classical linear regression within subjects, while the blue dots correspond to the shrunken “estimates” (technically predictions) you get for each subject from the mixed model.\n\nshrinkageplot(m2)\n\n\n\n\n\nMixedModels.PCA(m2)[:subj]\n\n\nPrincipal components based on correlation matrix\n (Intercept)  1.0    .\n days         0.08  1.0\n\nNormalized cumulative variances:\n[0.5407, 1.0]\n\nComponent loadings\n                PC1    PC2\n (Intercept)  -0.71  -0.71\n days         -0.71   0.71\n\n\n\nm2_zerocorr = fit(MixedModel, @formula(reaction ~ 1 + days + zerocorr(1 + days | subj)), sleepstudy)\n\n\n\n\n\nEst.\nSE\nz\np\nσ_subj\n\n\n\n\n(Intercept)\n251.4051\n6.7077\n37.48\n<1e-99\n24.1714\n\n\ndays\n10.4673\n1.5193\n6.89\n<1e-11\n5.7994\n\n\nResidual\n25.5561\n\n\n\n\n\n\n\n\n\n\nshrinkageplot(m2_zerocorr)\n\n\n\n\n\nMixedModels.PCA(m2)[:subj]\n\n\nPrincipal components based on correlation matrix\n (Intercept)  1.0    .\n days         0.08  1.0\n\nNormalized cumulative variances:\n[0.5407, 1.0]\n\nComponent loadings\n                PC1    PC2\n (Intercept)  -0.71  -0.71\n days         -0.71   0.71\n\n\nIf we consider a more complex model, then the change can be much more dramatic:\n\nkb07 = MixedModels.dataset(:kb07)\ncontrasts = Dict(:subj => Grouping(),\n                 :item => Grouping(),\n                 :spkr => EffectsCoding(),\n                 :prec => EffectsCoding(),\n                 :load => EffectsCoding() )\nm_kb07 = fit(MixedModel, @formula(rt_raw ~ 1 + spkr * prec * load + (1 + spkr + prec + load|subj) + (1 + spkr + prec + load|item)), kb07; contrasts)\n\n┌ Warning: ProgressMeter by default refresh meters with additional information in IJulia via `IJulia.clear_output`, which clears all outputs in the cell. \n│  - To prevent this behaviour, do `ProgressMeter.ijulia_behavior(:append)`. \n│  - To disable this warning message, do `ProgressMeter.ijulia_behavior(:clear)`.\n└ @ ProgressMeter /home/phillip/.julia/packages/ProgressMeter/sN2xr/src/ProgressMeter.jl:618\nMinimizing 601   Time: 0:00:00 ( 0.42 ms/it)\n  objective:  29652.62866326455\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_subj\nσ_item\n\n\n\n\n(Intercept)\n2225.5754\n84.9527\n26.20\n<1e-99\n328.0674\n393.4597\n\n\nspkr: old\n74.0912\n23.5388\n3.15\n0.0016\n43.1209\n45.1421\n\n\nprec: maintain\n-377.3128\n59.7116\n-6.32\n<1e-09\n118.9270\n302.2964\n\n\nload: yes\n102.0965\n24.6618\n4.14\n<1e-04\n49.7595\n58.4598\n\n\nspkr: old & prec: maintain\n-28.1656\n21.3820\n-1.32\n0.1878\n\n\n\n\nspkr: old & load: yes\n26.9646\n21.3821\n1.26\n0.2073\n\n\n\n\nprec: maintain & load: yes\n-18.5510\n21.3821\n-0.87\n0.3856\n\n\n\n\nspkr: old & prec: maintain & load: yes\n15.6366\n21.3820\n0.73\n0.4646\n\n\n\n\nResidual\n904.3472\n\n\n\n\n\n\n\n\n\n\n\nMixedModels.PCA(m_kb07)[:subj]\n\n\nPrincipal components based on correlation matrix\n (Intercept)      1.0    .     .     .\n spkr: old        1.0   1.0    .     .\n prec: maintain  -1.0  -1.0   1.0    .\n load: yes        1.0   1.0  -1.0   1.0\n\nNormalized cumulative variances:\n[1.0, 1.0, 1.0, 1.0]\n\nComponent loadings\n                   PC1    PC2    PC3    PC4\n (Intercept)     -0.5   -0.38  -0.33  -0.71\n spkr: old       -0.5   -0.38  -0.33   0.71\n prec: maintain   0.5    0.09  -0.86   0.0\n load: yes       -0.5    0.84  -0.2    0.0\n\n\nThe effective dimensionality can be seen in the way that the random effects collapse into lines (i.e.. a 1-D object) within the majority of the panels (each representing a 2-D plane).\n\nshrinkageplot(m_kb07, :subj)\n\n\n\n\n\nm_kb07zc = fit(MixedModel, @formula(rt_raw ~ 1 + spkr * prec * load + zerocorr(1 + spkr + prec + load | subj) + zerocorr(1 + spkr + prec + load | item)), kb07; contrasts)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_subj\nσ_item\n\n\n\n\n(Intercept)\n2225.6609\n85.7868\n25.94\n<1e-99\n330.9087\n397.6761\n\n\nspkr: old\n74.1345\n21.4927\n3.45\n0.0006\n0.0000\n0.0000\n\n\nprec: maintain\n-377.2695\n59.6939\n-6.32\n<1e-09\n113.3176\n303.1632\n\n\nload: yes\n102.0110\n22.9634\n4.44\n<1e-05\n46.3311\n29.4211\n\n\nspkr: old & prec: maintain\n-28.0802\n21.4927\n-1.31\n0.1914\n\n\n\n\nspkr: old & load: yes\n26.9213\n21.4927\n1.25\n0.2104\n\n\n\n\nprec: maintain & load: yes\n-18.5943\n21.4927\n-0.87\n0.3870\n\n\n\n\nspkr: old & prec: maintain & load: yes\n15.5512\n21.4927\n0.72\n0.4693\n\n\n\n\nResidual\n909.0068\n\n\n\n\n\n\n\n\n\n\n\nMixedModels.PCA(m_kb07zc)[:subj]\n\n\nPrincipal components based on correlation matrix\n (Intercept)     1.0  .    .    .\n spkr: old       0.0  0.0  .    .\n prec: maintain  0.0  0.0  1.0  .\n load: yes       0.0  0.0  0.0  1.0\n\nNormalized cumulative variances:\n[0.3333, 0.6667, 1.0, 1.0]\n\nComponent loadings\n                  PC1   PC2   PC3     PC4\n (Intercept)     1.0   0.0   0.0     0.0\n spkr: old       0.0   0.0   0.0   NaN\n prec: maintain  0.0   0.0   1.0     0.0\n load: yes       0.0   1.0   0.0     0.0\n\n\nWhen we force the correlations to be zero, we can no longer get diagonal lines – we we can only get horizontal or vertical lines within each panel. Diagonal lines correspond to non zeor correlations between two variance components.\n\nshrinkageplot(m_kb07zc, :subj)"
  },
  {
    "objectID": "02-shrinkage.html",
    "href": "02-shrinkage.html",
    "title": "Shrinkage and the correlation parameter",
    "section": "",
    "text": "Because the number of correlations grows quadratically with the number of random slopes, adding just one additional slope (whether main effect or interaction) can greatly increase the number of free parameters in the model. We can omit them from the model, using the || in lme4, splitting elements into separate (x + ... | grp) terms in lme4 or MixedModels.jl or using zerocorr in MixedModels.jl.\nIn terms of philosophy, this is a bit like omitting higher order interactions from the fixed effects: there is change in the bias-variance tradeoff. However, practice suggests that the tradeoff is often worthwhile, although it makes shrinkage less efficient. John Kruschke has a nice worked example on his blog.\nFor our sleep study example, we we can see that there is very little impact because there is almost no correlation between the random intercept and random slope.\n\nusing CairoMakie\nusing MixedModels\nusing MixedModelsMakie\n\nsleepstudy = MixedModels.dataset(:sleepstudy)\n# REML=false by default in Julia\nm2 = fit(MixedModel, @formula(reaction ~ 1 + days + (1 + days | subj)), sleepstudy)\n\n\n\n\n\nEst.\nSE\nz\np\nσ_subj\n\n\n\n\n(Intercept)\n251.4051\n6.6323\n37.91\n<1e-99\n23.7805\n\n\ndays\n10.4673\n1.5022\n6.97\n<1e-11\n5.7168\n\n\nResidual\n25.5918\n\n\n\n\n\n\n\n\n\nWe can see this with a shrinkage plot, which show the by-group (here: by-subject) offsets from the grand mean for each random effect. The red dots correspond to the esimtates you would get from classical linear regression within subjects, while the blue dots correspond to the shrunken “estimates” (technically predictions) you get for each subject from the mixed model.\n\nshrinkageplot(m2)\n\n\n\n\n\nMixedModels.PCA(m2)[:subj]\n\n\nPrincipal components based on correlation matrix\n (Intercept)  1.0    .\n days         0.08  1.0\n\nNormalized cumulative variances:\n[0.5407, 1.0]\n\nComponent loadings\n                PC1    PC2\n (Intercept)  -0.71  -0.71\n days         -0.71   0.71\n\n\n\nm2_zerocorr = fit(MixedModel, @formula(reaction ~ 1 + days + zerocorr(1 + days | subj)), sleepstudy)\n\n\n\n\n\nEst.\nSE\nz\np\nσ_subj\n\n\n\n\n(Intercept)\n251.4051\n6.7077\n37.48\n<1e-99\n24.1714\n\n\ndays\n10.4673\n1.5193\n6.89\n<1e-11\n5.7994\n\n\nResidual\n25.5561\n\n\n\n\n\n\n\n\n\n\nshrinkageplot(m2_zerocorr)\n\n\n\n\n\nMixedModels.PCA(m2)[:subj]\n\n\nPrincipal components based on correlation matrix\n (Intercept)  1.0    .\n days         0.08  1.0\n\nNormalized cumulative variances:\n[0.5407, 1.0]\n\nComponent loadings\n                PC1    PC2\n (Intercept)  -0.71  -0.71\n days         -0.71   0.71\n\n\nIf we consider a more complex model, then the change can be much more dramatic:\n\nkb07 = MixedModels.dataset(:kb07)\ncontrasts = Dict(:subj => Grouping(),\n                 :item => Grouping(),\n                 :spkr => EffectsCoding(),\n                 :prec => EffectsCoding(),\n                 :load => EffectsCoding() )\nm_kb07 = fit(MixedModel, @formula(rt_raw ~ 1 + spkr * prec * load + (1 + spkr + prec + load|subj) + (1 + spkr + prec + load|item)), kb07; contrasts)\n\n┌ Warning: ProgressMeter by default refresh meters with additional information in IJulia via `IJulia.clear_output`, which clears all outputs in the cell. \n│  - To prevent this behaviour, do `ProgressMeter.ijulia_behavior(:append)`. \n│  - To disable this warning message, do `ProgressMeter.ijulia_behavior(:clear)`.\n└ @ ProgressMeter /home/phillip/.julia/packages/ProgressMeter/sN2xr/src/ProgressMeter.jl:618\nMinimizing 601   Time: 0:00:00 ( 0.62 ms/it)\n  objective:  29652.62866326455\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_subj\nσ_item\n\n\n\n\n(Intercept)\n2225.5754\n84.9527\n26.20\n<1e-99\n328.0674\n393.4597\n\n\nspkr: old\n74.0912\n23.5388\n3.15\n0.0016\n43.1209\n45.1421\n\n\nprec: maintain\n-377.3128\n59.7116\n-6.32\n<1e-09\n118.9270\n302.2964\n\n\nload: yes\n102.0965\n24.6618\n4.14\n<1e-04\n49.7595\n58.4598\n\n\nspkr: old & prec: maintain\n-28.1656\n21.3820\n-1.32\n0.1878\n\n\n\n\nspkr: old & load: yes\n26.9646\n21.3821\n1.26\n0.2073\n\n\n\n\nprec: maintain & load: yes\n-18.5510\n21.3821\n-0.87\n0.3856\n\n\n\n\nspkr: old & prec: maintain & load: yes\n15.6366\n21.3820\n0.73\n0.4646\n\n\n\n\nResidual\n904.3472\n\n\n\n\n\n\n\n\n\n\n\nMixedModels.PCA(m_kb07)[:subj]\n\n\nPrincipal components based on correlation matrix\n (Intercept)      1.0    .     .     .\n spkr: old        1.0   1.0    .     .\n prec: maintain  -1.0  -1.0   1.0    .\n load: yes        1.0   1.0  -1.0   1.0\n\nNormalized cumulative variances:\n[1.0, 1.0, 1.0, 1.0]\n\nComponent loadings\n                   PC1    PC2    PC3    PC4\n (Intercept)     -0.5   -0.38  -0.33  -0.71\n spkr: old       -0.5   -0.38  -0.33   0.71\n prec: maintain   0.5    0.09  -0.86   0.0\n load: yes       -0.5    0.84  -0.2    0.0\n\n\nThe effective dimensionality can be seen in the way that the random effects collapse into lines (i.e.. a 1-D object) within the majority of the panels (each representing a 2-D plane).\n\nshrinkageplot(m_kb07, :subj)\n\n\n\n\n\nm_kb07zc = fit(MixedModel, @formula(rt_raw ~ 1 + spkr * prec * load + zerocorr(1 + spkr + prec + load | subj) + zerocorr(1 + spkr + prec + load | item)), kb07; contrasts)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_subj\nσ_item\n\n\n\n\n(Intercept)\n2225.6609\n85.7868\n25.94\n<1e-99\n330.9087\n397.6761\n\n\nspkr: old\n74.1345\n21.4927\n3.45\n0.0006\n0.0000\n0.0000\n\n\nprec: maintain\n-377.2695\n59.6939\n-6.32\n<1e-09\n113.3176\n303.1632\n\n\nload: yes\n102.0110\n22.9634\n4.44\n<1e-05\n46.3311\n29.4211\n\n\nspkr: old & prec: maintain\n-28.0802\n21.4927\n-1.31\n0.1914\n\n\n\n\nspkr: old & load: yes\n26.9213\n21.4927\n1.25\n0.2104\n\n\n\n\nprec: maintain & load: yes\n-18.5943\n21.4927\n-0.87\n0.3870\n\n\n\n\nspkr: old & prec: maintain & load: yes\n15.5512\n21.4927\n0.72\n0.4693\n\n\n\n\nResidual\n909.0068\n\n\n\n\n\n\n\n\n\n\n\nMixedModels.PCA(m_kb07zc)[:subj]\n\n\nPrincipal components based on correlation matrix\n (Intercept)     1.0  .    .    .\n spkr: old       0.0  0.0  .    .\n prec: maintain  0.0  0.0  1.0  .\n load: yes       0.0  0.0  0.0  1.0\n\nNormalized cumulative variances:\n[0.3333, 0.6667, 1.0, 1.0]\n\nComponent loadings\n                  PC1   PC2   PC3     PC4\n (Intercept)     1.0   0.0   0.0     0.0\n spkr: old       0.0   0.0   0.0   NaN\n prec: maintain  0.0   0.0   1.0     0.0\n load: yes       0.0   1.0   0.0     0.0\n\n\nWhen we force the correlations to be zero, we can no longer get diagonal lines – we we can only get horizontal or vertical lines within each panel. Diagonal lines correspond to non zeor correlations between two variance components.\n\nshrinkageplot(m_kb07zc, :subj)"
  },
  {
    "objectID": "02-shrinkage.html#what-does-omiting-the-correlation-parameter-do",
    "href": "02-shrinkage.html#what-does-omiting-the-correlation-parameter-do",
    "title": "Shrinkage and the correlation parameter",
    "section": "What does omiting the correlation parameter do?",
    "text": "What does omiting the correlation parameter do?\nBecause the number of correlations grows quadratically with the number of random slopes, adding just one additional slope (whether main effect or interaction) can greatly increase the number of free parameters in the model. We can omit them from the model, using the || in lme4, splitting elements into separate (x + ... | grp) terms in lme4 or MixedModels.jl or using zerocorr in MixedModels.jl.\nIn terms of philosophy, this is a bit like omitting higher order interactions from the fixed effects: there is change in the bias-variance tradeoff. However, practice suggests that the tradeoff is often worthwhile, although it makes shrinkage less efficient. John Kruschke has a nice worked example on his blog.\nFor our sleep study example, we we can see that there is very little impact because there is almost no correlation between the random intercept and random slope.\n\nusing CairoMakie\nusing MixedModels\nusing MixedModelsMakie\n\nsleepstudy = MixedModels.dataset(:sleepstudy)\n# REML=false by default in Julia\nm2 = fit(MixedModel, @formula(reaction ~ 1 + days + (1 + days | subj)), sleepstudy)\n\n\n\n\n\nEst.\nSE\nz\np\nσ_subj\n\n\n\n\n(Intercept)\n251.4051\n6.6323\n37.91\n<1e-99\n23.7805\n\n\ndays\n10.4673\n1.5022\n6.97\n<1e-11\n5.7168\n\n\nResidual\n25.5918\n\n\n\n\n\n\n\n\n\nWe can see this with a shrinkage plot, which show the by-group (here: by-subject) offsets from the grand mean for each random effect. The red dots correspond to the esimtates you would get from classical linear regression within subjects, while the blue dots correspond to the shrunken “estimates” (technically predictions) you get for each subject from the mixed model.\n\nshrinkageplot(m2)\n\n\n\n\n\nMixedModels.PCA(m2)[:subj]\n\n\nPrincipal components based on correlation matrix\n (Intercept)  1.0    .\n days         0.08  1.0\n\nNormalized cumulative variances:\n[0.5407, 1.0]\n\nComponent loadings\n                PC1    PC2\n (Intercept)  -0.71  -0.71\n days         -0.71   0.71\n\n\n\nm2_zerocorr = fit(MixedModel, @formula(reaction ~ 1 + days + zerocorr(1 + days | subj)), sleepstudy)\n\n\n\n\n\nEst.\nSE\nz\np\nσ_subj\n\n\n\n\n(Intercept)\n251.4051\n6.7077\n37.48\n<1e-99\n24.1714\n\n\ndays\n10.4673\n1.5193\n6.89\n<1e-11\n5.7994\n\n\nResidual\n25.5561\n\n\n\n\n\n\n\n\n\n\nshrinkageplot(m2_zerocorr)\n\n\n\n\n\nMixedModels.PCA(m2)[:subj]\n\n\nPrincipal components based on correlation matrix\n (Intercept)  1.0    .\n days         0.08  1.0\n\nNormalized cumulative variances:\n[0.5407, 1.0]\n\nComponent loadings\n                PC1    PC2\n (Intercept)  -0.71  -0.71\n days         -0.71   0.71\n\n\nIf we consider a more complex model, then the change can be much more dramatic:\n\nkb07 = MixedModels.dataset(:kb07)\ncontrasts = Dict(:subj => Grouping(),\n                 :item => Grouping(),\n                 :spkr => EffectsCoding(),\n                 :prec => EffectsCoding(),\n                 :load => EffectsCoding() )\nm_kb07 = fit(MixedModel, @formula(rt_raw ~ 1 + spkr * prec * load + (1 + spkr + prec + load|subj) + (1 + spkr + prec + load|item)), kb07; contrasts)\n\n┌ Warning: ProgressMeter by default refresh meters with additional information in IJulia via `IJulia.clear_output`, which clears all outputs in the cell. \n│  - To prevent this behaviour, do `ProgressMeter.ijulia_behavior(:append)`. \n│  - To disable this warning message, do `ProgressMeter.ijulia_behavior(:clear)`.\n└ @ ProgressMeter /home/phillip/.julia/packages/ProgressMeter/sN2xr/src/ProgressMeter.jl:618\nMinimizing 601   Time: 0:00:00 ( 0.47 ms/it)\n  objective:  29652.62866326455\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_subj\nσ_item\n\n\n\n\n(Intercept)\n2225.5754\n84.9527\n26.20\n<1e-99\n328.0674\n393.4597\n\n\nspkr: old\n74.0912\n23.5388\n3.15\n0.0016\n43.1209\n45.1421\n\n\nprec: maintain\n-377.3128\n59.7116\n-6.32\n<1e-09\n118.9270\n302.2964\n\n\nload: yes\n102.0965\n24.6618\n4.14\n<1e-04\n49.7595\n58.4598\n\n\nspkr: old & prec: maintain\n-28.1656\n21.3820\n-1.32\n0.1878\n\n\n\n\nspkr: old & load: yes\n26.9646\n21.3821\n1.26\n0.2073\n\n\n\n\nprec: maintain & load: yes\n-18.5510\n21.3821\n-0.87\n0.3856\n\n\n\n\nspkr: old & prec: maintain & load: yes\n15.6366\n21.3820\n0.73\n0.4646\n\n\n\n\nResidual\n904.3472\n\n\n\n\n\n\n\n\n\n\n\nMixedModels.PCA(m_kb07)[:subj]\n\n\nPrincipal components based on correlation matrix\n (Intercept)      1.0    .     .     .\n spkr: old        1.0   1.0    .     .\n prec: maintain  -1.0  -1.0   1.0    .\n load: yes        1.0   1.0  -1.0   1.0\n\nNormalized cumulative variances:\n[1.0, 1.0, 1.0, 1.0]\n\nComponent loadings\n                   PC1    PC2    PC3    PC4\n (Intercept)     -0.5   -0.38  -0.33  -0.71\n spkr: old       -0.5   -0.38  -0.33   0.71\n prec: maintain   0.5    0.09  -0.86   0.0\n load: yes       -0.5    0.84  -0.2    0.0\n\n\nThe effective dimensionality can be seen in the way that the random effects collapse into lines (i.e.. a 1-D object) within the majority of the panels (each representing a 2-D plane).\n\nshrinkageplot(m_kb07, :subj)\n\n\n\n\n\nm_kb07zc = fit(MixedModel, @formula(rt_raw ~ 1 + spkr * prec * load + zerocorr(1 + spkr + prec + load | subj) + zerocorr(1 + spkr + prec + load | item)), kb07; contrasts)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_subj\nσ_item\n\n\n\n\n(Intercept)\n2225.6609\n85.7868\n25.94\n<1e-99\n330.9087\n397.6761\n\n\nspkr: old\n74.1345\n21.4927\n3.45\n0.0006\n0.0000\n0.0000\n\n\nprec: maintain\n-377.2695\n59.6939\n-6.32\n<1e-09\n113.3176\n303.1632\n\n\nload: yes\n102.0110\n22.9634\n4.44\n<1e-05\n46.3311\n29.4211\n\n\nspkr: old & prec: maintain\n-28.0802\n21.4927\n-1.31\n0.1914\n\n\n\n\nspkr: old & load: yes\n26.9213\n21.4927\n1.25\n0.2104\n\n\n\n\nprec: maintain & load: yes\n-18.5943\n21.4927\n-0.87\n0.3870\n\n\n\n\nspkr: old & prec: maintain & load: yes\n15.5512\n21.4927\n0.72\n0.4693\n\n\n\n\nResidual\n909.0068\n\n\n\n\n\n\n\n\n\n\n\nMixedModels.PCA(m_kb07zc)[:subj]\n\n\nPrincipal components based on correlation matrix\n (Intercept)     1.0  .    .    .\n spkr: old       0.0  0.0  .    .\n prec: maintain  0.0  0.0  1.0  .\n load: yes       0.0  0.0  0.0  1.0\n\nNormalized cumulative variances:\n[0.3333, 0.6667, 1.0, 1.0]\n\nComponent loadings\n                  PC1   PC2   PC3     PC4\n (Intercept)     1.0   0.0   0.0     0.0\n spkr: old       0.0   0.0   0.0   NaN\n prec: maintain  0.0   0.0   1.0     0.0\n load: yes       0.0   1.0   0.0     0.0\n\n\nWhen we force the correlations to be zero, we can no longer get diagonal lines – we we can only get horizontal or vertical lines within each panel. Diagonal lines correspond to non zeor correlations between two variance components.\n\nshrinkageplot(m_kb07zc, :subj)"
  }
]